LLMs were RL'd a lot during their training which is inherently a goal oriented process tha itself probably made
  them more goal minded

Besides all this it's well documented that current LLMs don't explore enough, are lacking (as of Jan 2026) in the memory department, suffering from context rot issues. It is getting better but we still have to account for it.

In an AGI ideal world you won't have to verify anything and you just give a goal and come back to a perfect result. In reality it may never get there. Swarms of humans are as close to AGI as there is currently and yet they too have processes and structures that admit the individual isn't perfect. Hence why there are development methodologies and qa fix cycles during this development cycle. If we take humans as a state to strive towards (in the mean time, while machines are still behind) we can probably assume we'll need this for the forrseeable future.
