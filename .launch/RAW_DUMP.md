LLMs were RL'd a lot during their training which is inherently a goal oriented process tha itself probably made
  them more goal minded

They are so smart today that tinkering too much with thier process is likely to throw them offf. For example
The approach of planning an implementation down to code or even exact type level may backfire. Even if you are the smartest domain expert you as a human simply cannot hold in your mind all the nuances of the situation and even worse some of them are not even known before you are already deep inside the implementation process. Like a human would the llm needs flexibility to adapt or it will do what it does best - listen to you and try to fulfill the goals you set it with. If those goals are too rigid it, and a problem or nuance surfaces that's where you'll see things like it using any types and trying to put ts or eslint ignore rules etc basically trying to bend the reality around your goal in areas that were less precisely specified by you. A plan needs to be adaptable and for that in my mind you shouldn't even call it a plan but an "initial approach". In an ideal world (maybe when LLMs are smarter) this will become irrelevant even with the current trends. The will just be able to take your goal and get there in a nice way. Today while they are smart they still may get lost depending on the complexity. Your job as a human is first and foremost to define the acceptance criteria and goals (and constraints) for the llm. And that initial approach. Key word initial. The llm has to have wiggle room to scrap that approach completely if reality in th ground demands it. Like the famous saying "everybody's got a plan until they get punched in the face".

Besides all this it's well documented that current LLMs don't explore enough, are lacking (as of Jan 2026) in the memory department, suffering from context rot issues. It is getting better but we still have to account for it.

In an AGI ideal world you won't have to verify anything and you just give a goal and come back to a perfect result. In reality it may never get there. Swarms of humans are as close to AGI as there is currently and yet they too have processes and structures that admit the individual isn't perfect. Hence why there are development methodologies and qa fix cycles during this development cycle. If we take humans as a state to strive towards (in the mean time, while machines are still behind) we can probably assume we'll need this for the forrseeable future.
